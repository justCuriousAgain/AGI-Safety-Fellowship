# Insights on AI Threat Models and relevant approaches

Link: <https://www.eacambridge.org/agi-week-3>

## Exercises

* Christiano’s “influence-seeking systems” threat model in What Failure Looks Like is in some ways analogous to profit-seeking companies. What are the most important mechanisms preventing companies from catastrophic misbehaviour? Which of those would and wouldn’t apply to influence-seeking AIs?

* What are the individual tasks involved in machine learning research (or some other type of research important for technological progress)? Identify the parts of the process which have already been automated, the parts of the process which seem like they could plausibly soon be automated, and the parts of the process which seem hardest to automate.

> Identifying what problem to solve is hard to automate. But then that goes meta. For instance, given a prompt: "I want to maximise sales" you might be able to build a system that develops strategies using data to maximize sales. Hard, but not unachievable. But Maximise Sales is the thing to do is something difficult to automate. Even if were feasible, it'd be scary to live in a world where a machine is making those decisions for you.

## Discussion prompts

* What are the biggest vulnerabilities in human civilisation that might be exploited by misaligned AGIs? To what extent do they depend on the development of other technologies more powerful than those which exist today?

>* Points:
>   * (Sudhanshu) Humans don’t agree on things / all humans don’t agree on any single thing
>   * (GC) Technological vulnerabilities that can be exploited
>   * (CL) Social hacks / social engineering / deep fakes / creating + disseminating info hazards that can be exploited
>   * (PA) Exploiting individual needs
>   * (Max) We are biologically vulnerable
  
* Does the distinction between “paying the alignment tax” and “reducing the alignment tax” make sense to you? Give a concrete example of each case. Are there activities which fall into both of these categories, or are ambiguous between them?

> Need to watch this first: Current work in AI alignment | Paul Christiano | EA Global: San Francisco 2019 <https://www.youtube.com/watch?v=-vsYtevJ2bc>

* Most of the readings so far have been framed in the current paradigm of deep learning. Is this reasonable? To what extent are they undermined by the possibility of future paradigm shifts in AI?

> Some relevant links:
>
> * This might give some basic idea for immune system analogy for AI defenses: <https://youtu.be/u2qRUtg2k3Y>
> * <https://youtu.be/BO8yFzA8gQ0>
> * Quantum Artificial General Intelligence, Tim Ferriss & Eric Schmidt <https://youtu.be/VFuElWbRuHM>
> * Robert Miles on the Alignment problem: <https://www.youtube.com/watch?v=pYXy-A4siMw&ab_channel=RobertMiles>

My thoughts:

> What if we come up with a new paradigm of thinking where getting the aligned AI is an easier thing than an unaligned one. The current approach of Agent-based modeling leads to similar problems we face as humans. What if we could model AI not as agents trying to achieve goals but something else.
>
> For example, just thought of a prompt. This relates to the book I read recently called Atomic Habits. It talks about something interesting: Our habits are not fundamentally good or bad, they're just optimizations for certain goals our body has figured are important given our lifestyle. It does sound to me like a mesa-optimizer situation. Also, he proposes that it's not the goals that make someone successful, coz the winner and loser in a competition both have the same goals. It's the habits. The identity. You won't be able to achieve your goals, if the habits you're trying to build conflict with your identity. So, if you think you're a lazy person, making a running habit won't go far because for your brain "a lazy person doesn't do running". Change your identity. If you think of yourself as a person who's a doer, your body and brain will automatically optimize so that it becomes hard for you to leave things undone.
>
> My question. Say you had the chance to give the AI an identity. Not goals, but an identity. What'd you give?
