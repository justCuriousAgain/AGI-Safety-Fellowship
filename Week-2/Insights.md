# Insights on Specification Gaming

Link: <https://www.eacambridge.org/agi-week-2>

## Exercises

* By some definitions, a chess AI has the goal of winning. When is it useful to describe it that way? What are the key differences between human goals and the “goals” of a chess AI?

* Why is it not appropriate to describe the agents from Krakovna et al. (2020) as displaying inner alignment failures?

> Relevant links:
>
> * Rob Miles videos:
>   * <https://www.youtube.com/watch?v=v9M2Ho9I9Qo&t=1s>
>   * <https://www.youtube.com/watch?v=ZeecOKBus3Q&ab_channel=RobertMiles>
>   * <https://www.youtube.com/watch?v=nKJlF-olKmg&ab_channel=RobertMiles>
>   * <https://www.youtube.com/watch?v=JRuNA2eK7w0&ab_channel=RobertMiles>
>   * <https://www.youtube.com/watch?v=L5pUA3LsEaw&ab_channel=RobertMiles>
>   * <https://www.youtube.com/watch?v=hEUO6pjwFOo&ab_channel=RobertMiles>
>   * <https://www.youtube.com/watch?v=ZeecOKBus3Q&ab_channel=RobertMiles>
>   * <https://www.youtube.com/watch?v=ZeecOKBus3Q&ab_channel=RobertMiles>
>   * <https://www.youtube.com/watch?v=PYylPRX6z4Q&ab_channel=RobertMiles>

## Discussion prompts

* Christiano (2018) defined alignment as follows: “an AI A is aligned with an operator H if A is trying to do what H wants it to do”. Some questions about this:

* What’s the most natural way to interpret “what the human wants” - what they say, or what they think, or what they would think if they thought about it for much longer?

* How should we define an AI being aligned to a group of humans, rather than an individual?

* Does it make sense to talk about corporations and countries having goals? Does it matter that these consist of many different people, or can we treat them as agents with goals in a similar way to individual humans?

* By some definitions, a chess AI has the goal of winning. When is it useful to describe it that way? What are the key differences between human goals and the “goals” of a chess AI?

* The same questions, but for corporations and countries instead of chess AIs. Does it matter that these consist of many different people, or can we treat them as agents with goals in a similar way to individual humans?

* To what extent are humans inner misaligned with respect to evolution? How can you tell, and what might similar indicators look like in AGIs?

* Did Bostrom miss any important convergent instrumental goals? (His current list: self-preservation, goal-content integrity, cognitive enhancement, technological perfection, resource acquisition.) One way of thinking about this might be to consider which goals humans regularly pursue and why.

* Suppose that we want to build a highly intelligent AGI that is myopic, in the sense that it only cares about what happens over the next day or week. Would such an agent still have convergent instrumental goals? What factors might make it easier or harder to train a myopic AGI than a non-myopic AGI?

> My thoughts:
> How can adversarial training be used to address the problem of inner and outer misalignment or specification gaming and the problem of distributional shifts?
> Distribution shift can allow the agent to figure out the difference between the training and the deployed environment and it can lead to deceptive alignment
