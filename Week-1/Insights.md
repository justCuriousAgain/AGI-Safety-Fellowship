# Insights from readings and questions

Link: <https://www.eacambridge.org/agi-week-1>

## Exercises

* A crucial feature of AGI is that it will possess cognitive skills which are useful across a range of tasks, rather than just the tasks it was trained to perform. Which cognitive skills did humans evolve because they were useful in our ancestral environments, which have remained useful in our modern environment? Which have become less useful?

> Relevant:
    >>
    >> Language (Still relevant)
>
> Not always relevant:
    >>
    >> Making tools (Relevant but maybe not so much on an individual level)
>

* What are the most plausible ways for the hypothesis “we will eventually build AGIs which have transformative impacts on the world” to be false? How likely are they?

> Something like Neuralink/developments in Gene editing can be a plausible way to make it false.

## Discussion prompts

* Ngo (2020) opens with a definition of intelligence as the ability to perform well at a wide range of cognitive tasks. What are some advantages and disadvantages of this definition?

* Here’s an alternative way of describing general intelligence: whatever mental skills humans have that allow us to build technology and civilization (in contrast to other animals). What do you think about this characterisation?

* One intuition for how to think about very smart AIs: imagine speeding up human intellectual development by a factor of X. What do you expect a human civilization to know by 2100 or 2200? If an AI could do the same quality of research, but 10 or 100 times faster, how would you use it?

* How frequently do humans build technologies where some of the details of why they work aren’t understood by anyone? Would it be very surprising if we built AGI without understanding very much about how its thinking process works?

* Thinking about AGI involves reasoning about entities smarter than us, and a future technology that doesn't exist yet. What problems does this introduce, and how should we respond to them?